# config 템플릿 (수정 X)
name: 이름
seed: 42
train:
  model_name: 허깅페이스 모델명
  epoch: 100
  batch_size: 32
  LR: 
    name: 스케줄러명 # LambdaLR, StepLR, CyclicLR, ExponentialLR, WarmupConstantLR , WarmupDecayLR
    lr: 0.000001
    base: 20 # CyclicLR를 쓴다면, LR에 대한 min_lr을 적어주세요(20 == LR/20)
    max: 5 # CyclicLR를 쓴다면, LR에 대한 max_lr을 적어주세요(5 == LR/5)
    step_up: 5 # CyclicLR를 쓴다면, warmup steps를 적어주세요
    step_down: 5 # CyclicLR를 쓴다면, cooldown steps를 적어주세요 (단, up+down은 epoch과 동일해야 함)
    warmupconstantLR_step: 3
    warmupdecayLR_warmup:  400 # step 기준으로 계산
    warmupdecayLR_total:  4200 # step 기준으로 계산
    interval: step # epoch
  lossF:
    name: CrossEntropyLoss # focal_loss
    focal_loss_scale: 0.5
    smooth_scale: 0.0 # 일반 CE 쓰고싶을 경우 0.0
  optim: 옵티마이저 함수명
  shuffle: True
  token_max_len: 100
  patience: 5
  test_size: 0.2
  save_top_k: 3 # Inference할 top k개의 모델.
  type_classify: False
  adverse_valid: False #
  no_valid: False # valid set 안 씀
  TAPT: False #
  LSTM:
    Do: True
    truncate: True
  halfprecision: True
select_DC:
  - remove_duplicated
  # - normalize_class
  # - add_entity_tokens_base
  # - add_entity_tokens_detail
  - add_only_punct
  - add_others_tokens
  - quering_with_punct
select_DA:
  # - swap_sentence
  # - sub_obj_change_augment
wandb:
  id: 아이디
option:
  early_stop: True
  short_tokenizing: False
