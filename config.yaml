# config 템플릿 (수정 X)
name: 이름
seed: 42
train:
  model_name: 허깅페이스 모델명
  epoch: 100
  batch_size: 32
  LR:    #LambdaLR, StepLR, CyclicLR, ExponentialLR, WarmupConstantLR 
    name: 스케줄러명
    lr: 0.000001
    base: 20 # CyclicLR를 쓴다면, LR에 대한 min_lr을 적어주세요(20 == LR/20)
    max: 5 # CyclicLR를 쓴다면, LR에 대한 max_lr을 적어주세요(5 == LR/5)
    step_up: 5 # CyclicLR를 쓴다면, warmup epoch를 적어주세요
    step_down: 5 # CyclicLR를 쓴다면, cooldown epoch를 적어주세요 (단, up+down은 epoch과 동일해야 함)
    warmupconstantLR_step: 3
  optim: 옵티마이저 함수명
  shuffle: True
  token_max_len: 100
  patience: 5
  test_size: 0.2
  save_top_k: 3 # Inference할 top k개의 모델.
  type_classify: False
  focal_loss: True # focal_loss를 사용한다면 True, False시, 자동으로 주어진 lossF로 갑니다
  focal_loss_scale: 1 # focal_loss의 scale을 조정합니다
  adverse_valid: False # 
  TAPT: False #
  LSTM:
    Do: True
    Truncate: True
select_DC:
  - remove_duplicated
  # - normalize_class
  # - add_entity_tokens_base
  # - add_entity_tokens_detail
  - add_only_punct
  - add_others_tokens
  - quering_with_punct
select_DA:
wandb:
  id: 아이디
option:
  early_stop: True
  short_tokenizing: False